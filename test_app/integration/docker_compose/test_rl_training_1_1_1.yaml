version: '3'
services:
  server:
    volumes:
      - ./config.py:/fed-flow/app/config/config.py
      - ./../../../Graphs:/Graphs
      - ./../../../agent:/app/agent
    image: fed-flow:test
    deploy:
      resources:
        limits:
          cpus: '3'
        reservations:
          cpus: '3'
      restart_policy:
        condition: on-failure
    ports:
      - "5002:5002"
    hostname: server
    working_dir: /fed-flow/app/rl_training/runner
    entrypoint: bash -c "python3 /fed-flow/app/rl_training/runner/rl_server_runner.py -o True -s edge_based_rl_splitting -e True"
  edge1:
    volumes:
      - ./config.py:/fed-flow/app/config/config.py
    image: fed-flow:test
    deploy:
      resources:
        limits:
          cpus: '1'
        reservations:
          cpus: '1'
      restart_policy:
        condition: on-failure
    ports:
      - "5001:5001"
    hostname: edge1
    working_dir: /fed-flow/app/rl_training/runner
    depends_on:
      - server
    entrypoint: bash -c "python3 /fed-flow/app/rl_training/runner/rl_edgeserver_runner.py -i 0 -o True -s edge_based_rl_splitting -e True"

  client1:
    volumes:
      - ./config.py:/fed-flow/app/config/config.py
    image: fed-flow:test
    deploy:
      resources:
        limits:
          cpus: '0.5'
        reservations:
          cpus: '0.5'
      restart_policy:
        condition: on-failure
    hostname: client1
    working_dir: /fed-flow/app/rl_training/runner
    depends_on:
      - edge1
    entrypoint: bash -c "python3 /fed-flow/app/rl_training/runner/rl_client_runner.py -i 0 -o True -s edge_based_rl_splitting -e True  & python3 /fed-flow/energy/service.py & wait -n"